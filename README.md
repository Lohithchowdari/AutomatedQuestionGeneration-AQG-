# ğŸš€ Automated Question Generation (AQG)  

Automated Question Generation (AQG) is an **AI-driven system** that automatically generates **multiple-choice, fill-in-the-blank, and short-answer questions** from text. It is useful for **educational assessments, quizzes, and NLP applications**, significantly improving efficiency in content creation.  

---

## ğŸ“– Project Overview  
This project focuses on **automated question generation** from **PDF documents, scripts, or lessons**. Using **Transformer models (T5, BART, and GPT-3)**, it processes text and generates meaningful **questions and answers** for learning and assessment purposes.  

---

## âš™ï¸ Features  
âœ… Supports **PDFs, scripts, or text-based lessons** as input.  
âœ… Uses **T5, BART, and OpenAI GPT models** for question generation.  
âœ… Fine-tunes **T5 on the MS MARCO dataset** for better accuracy.  
âœ… Supports **multiple NLP-based question generation techniques**.  
âœ… Saves and loads trained models for **efficient reusability**.  

---

## ğŸ›  Models Used  

| Model | Purpose |  
|--------|---------|  
| **T5 (Google's Transformer)** | Fine-tuned for **question generation** on the `MS MARCO` dataset. |  
| **BART (Facebook's Transformer)** | Generates questions based on **context using beam search**. |  
| **GPT-3 (OpenAI text-davinci-003)** | Generates **creative and diverse** questions using OpenAIâ€™s API. |  

---

## ğŸ“ Training Process  

### **1ï¸âƒ£ Dataset Used**  
- The model is trained using the **MS MARCO (Microsoft Machine Reading Comprehension)** dataset.  
- This dataset contains **passage-question-answer pairs**, making it ideal for **question generation** tasks.  

### **2ï¸âƒ£ Data Preprocessing**  
- Extracts text from **PDFs, scripts, or raw documents**.  
- Tokenizes text using **NLTK / SpaCy**.  
- Converts text into **input-output pairs** (`context â†’ question`).  
- Formats the dataset to match **T5/BARTâ€™s training structure**.  

### **3ï¸âƒ£ Model Fine-Tuning**  

#### **ğŸ”¹ T5 Fine-Tuning**  
âœ… Trained on **passage-based question generation tasks**.  
âœ… Uses **encoder-decoder Transformer architecture**.  
âœ… Fine-tuned using **Hugging Face Transformers**.  
âœ… **Loss function**: Cross-Entropy Loss.  

#### **ğŸ”¹ BART Fine-Tuning**  
âœ… Uses **beam search decoding** for better fluency.  
âœ… Optimized to generate **diverse and grammatically correct questions**.  

#### **ğŸ”¹ GPT-3 Integration**  
âœ… Generates **creative & complex** questions.  
âœ… Uses **OpenAIâ€™s API** for inference (without explicit fine-tuning).  

### **4ï¸âƒ£ Training Hyperparameters**  

| Parameter | Value |  
|-----------|------|  
| **Batch Size** | `4` |  
| **Epochs** | `3` |  
| **Learning Rate** | `2e-5` |  
| **Optimizer** | AdamW |  
| **Loss Function** | Cross-Entropy |  
| **Framework** | PyTorch + Hugging Face Transformers |  

### **5ï¸âƒ£ Model Evaluation**  
- **Evaluation Metrics**: BLEU, ROUGE, METEOR scores.  
- Compares **generated vs. ground-truth questions**.  
- **Visualizes** training loss & model performance.  

### **6ï¸âƒ£ Saving & Loading Models**  

#### **ğŸ”¹ Saving the Model**  
After training, you can save the model using:  python save.py

ğŸ“Š Expected Output
âœ” Model-generated questions from input text.
âœ” Fine-tuning logs and training loss graphs.
âœ” Saved model available for future use (./fine_tuned_t5_model/).

ğŸ”¥ Future Improvements
ğŸ”¹ Improve accuracy using hybrid Transformer-based ensembles.
ğŸ”¹ Add support for multiple datasets (SQuAD, HotpotQA).
ğŸ”¹ Develop a Streamlit Web App for an interactive user interface.
